{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import jieba\n",
    "from zhconv import convert\n",
    "from torch.autograd import Variable\n",
    "from utils.utils_sensation_lcsts import *\n",
    "from seq2seq.sensation_scorer import SensationCNN\n",
    "from utils.utils_sensation import input_txt_to_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CNN Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, dev, test, lang, max_q, max_r = prepare_data_seq(batch_size=128,\n",
    "                                                        debug=False, shuffle=True,\n",
    "                                                        pointer_gen=True,\n",
    "                                                        output_vocab_size=50000,\n",
    "                                                        thd=0.0)\n",
    "\n",
    "opts = torch.load(\"save/sensation/512_0.9579935073852539/args.th\")\n",
    "sensation_model = SensationCNN(opts, lang)\n",
    "checkpoint = torch.load(\"save/sensation/512_0.9579935073852539/sensation_scorer.th\")\n",
    "sensation_model.load_state_dict(checkpoint['model'])\n",
    "sensation_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yusan_data_path = 'dataset/yushan/eliminated_yushan_dataset.txt'\n",
    "yusan_title = []\n",
    "with open(yusan_data_path, 'r', encoding='utf8') as f:\n",
    "     for line in f.readlines():\n",
    "            if line != '\\n':\n",
    "                title, article = line.split('\\t')\n",
    "                yusan_title.append(convert(title, 'zh-cn'))\n",
    "            \n",
    "inputs = input_txt_to_batch(yusan_title, lang).cuda()\n",
    "yusan_title_score = sensation_model(inputs).cpu().detach().numpy()\n",
    "print(yusan_title_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without score\n",
    "results = ''\n",
    "with open(yusan_data_path, 'r', encoding='utf8') as f:\n",
    "     for i, line in enumerate(f.readlines()):\n",
    "        if line != '\\n':\n",
    "            title, article = line.split('\\t')\n",
    "            processed_title = (\" \".join(list(jieba.cut(convert(title, 'zh-cn')))))\n",
    "            processed_article = (\" \".join(list(jieba.cut(convert(article, 'zh-cn')))))\n",
    "            results += (processed_title + '\\t' + processed_article)\n",
    "\n",
    "with open('dataset/yushan/yushan_processed_data_without_score.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with score\n",
    "results = ''\n",
    "i = 0\n",
    "with open(yusan_data_path, 'r', encoding='utf8') as f:\n",
    "     for line in f.readlines():\n",
    "        if line != '\\n' and i < 324:\n",
    "            title, article = line.split('\\t')\n",
    "            processed_title = (\" \".join(list(jieba.cut(convert(title, 'zh-cn')))))\n",
    "            processed_article = (\" \".join(list(jieba.cut(convert(article, 'zh-cn')))))\n",
    "            results += (processed_title + '\\t' + str(yusan_title_score[i]) + '\\t' + processed_article)\n",
    "            i+=1\n",
    "        \n",
    "with open('dataset/yushan/yushan_processed_data_with_score.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset/yushan/yushan_processed_data_with_score.txt', 'r', encoding='utf8') as f:\n",
    "    for i in f.readlines():\n",
    "        if i != '\\n':\n",
    "            data.append(i.strip())\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, test_pairs = train_test_split(data,test_size=0.05,random_state=42)\n",
    "train_pairs, dev_pairs = train_test_split(train_pairs,test_size=0.075,random_state=42)\n",
    "print(len(dev_pairs), len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '\\n'.join(train_pairs)\n",
    "with open('dataset/yushan/same_train.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(train_data)\n",
    "\n",
    "valid_data = '\\n'.join(dev_pairs)\n",
    "with open('dataset/yushan/same_dev.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(valid_data)\n",
    "    \n",
    "test_data = '\\n'.join(test_pairs)\n",
    "with open('dataset/yushan/same_test.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset/yushan/yushan_processed_data_without_score.txt', 'r', encoding='utf8') as f:\n",
    "    for i in f.readlines():\n",
    "        if i != '\\n':\n",
    "            data.append(i.strip())\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, test_pairs = train_test_split(data,test_size=0.05,random_state=42)\n",
    "train_pairs, dev_pairs = train_test_split(train_pairs,test_size=0.075,random_state=42)\n",
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '\\n'.join(train_pairs)\n",
    "with open('dataset/yushan/segment_train.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(train_data)\n",
    "\n",
    "valid_data = '\\n'.join(dev_pairs)\n",
    "with open('dataset/yushan/segment_valid.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(valid_data)\n",
    "    \n",
    "test_data = '\\n'.join(test_pairs)\n",
    "with open('dataset/yushan/segment_test.txt', 'w+', encoding='utf8') as f:\n",
    "    f.write(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
